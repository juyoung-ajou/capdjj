# backend/rag_core.py

import os
from dotenv import load_dotenv

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ (ì—ëŸ¬ ë°©ì§€ ì²˜ë¦¬)
try:
    from langchain_openai import ChatOpenAI
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain_community.retrievers import BM25Retriever
    import chromadb
except ImportError as e:
    print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
    raise e

load_dotenv()

class RAGService:
    def __init__(self):
        print(" [ì‹œìŠ¤í…œ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25 + Vector) ì—”ì§„ ë¡œë”© ì¤‘...")
        
        # 1. [Triplet Loss ì›ë¦¬] ìž„ë² ë”© ëª¨ë¸
        self.embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
        
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        self.persist_directory = "chroma_db"
        self.collection_name = "rag_collection"
        
        self.client = chromadb.PersistentClient(path=self.persist_directory)
        self.vector_store = None
        
        # ì•™ìƒë¸”ì„ ìœ„í•œ ê°œë³„ ê²€ìƒ‰ê¸°
        self.bm25_retriever = None
        self.chroma_retriever = None

        try:
            # DB ì—°ê²°
            self.client.get_collection(name=self.collection_name)
            self.vector_store = Chroma(
                client=self.client,
                collection_name=self.collection_name,
                embedding_function=self.embeddings,
            )
            
            # [BM25 & TF-IDF ì›ë¦¬] í‚¤ì›Œë“œ ê²€ìƒ‰ê¸° êµ¬ì¶•
            print(" [ì‹œìŠ¤í…œ] BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ë° MMR ê²€ìƒ‰ê¸° ì¤€ë¹„...")
            
            existing_data = self.vector_store.get()
            all_docs = existing_data["documents"]
            metadatas = existing_data["metadatas"]
            
            if not all_docs:
                print(" [ê²½ê³ ] DBê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤.")
                return

            from langchain_core.documents import Document
            doc_objects = []
            for t, m in zip(all_docs, metadatas):
                if m is None: m = {}
                doc_objects.append(Document(page_content=t, metadata=m))
            
            if doc_objects:
                # 1. BM25 ê²€ìƒ‰ê¸° (í‚¤ì›Œë“œ ë§¤ì¹­ - TF-IDF í™•ë¥  í†µê³„)
                self.bm25_retriever = BM25Retriever.from_documents(doc_objects)
                self.bm25_retriever.k = 5
                
                # 2. Vector ê²€ìƒ‰ê¸° (MMR ì›ë¦¬ - ë‹¤ì–‘ì„± í™•ë³´)
                self.chroma_retriever = self.vector_store.as_retriever(
                    search_type="mmr", 
                    search_kwargs={"k": 5, "fetch_k": 20, "lambda_mult": 0.6}
                )
                
                print(f" [ì‹œìŠ¤í…œ] ì•™ìƒë¸”(Ensemble) ê²€ìƒ‰ ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ! (BM25 + MMR)")
            
        except Exception as e:
            print(f" [ì˜¤ë¥˜] ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def get_answer(self, query: str):
        if self.bm25_retriever is None or self.chroma_retriever is None:
            return {"answer": "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "sources": [], "context": ""}
        
        try:
            # [í•µì‹¬] ì•™ìƒë¸”(Ensemble) ë¡œì§ - RRF ë°©ì‹ ì‘ìš©
            # 1. í‚¤ì›Œë“œ ê²€ìƒ‰ (BM25)
            bm25_docs = self.bm25_retriever.invoke(query)
            
            # 2. ì˜ë¯¸ ê²€ìƒ‰ (Vector + MMR)
            vector_docs = self.chroma_retriever.invoke(query)
            
            # 3. ê²°ê³¼ ì„žê¸° (Vector 1ë“± -> BM25 1ë“± -> Vector 2ë“± -> ...)
            combined_docs = []
            seen_contents = set()
            
            max_len = max(len(bm25_docs), len(vector_docs))
            for i in range(max_len):
                if i < len(vector_docs):
                    doc = vector_docs[i]
                    if doc.page_content not in seen_contents:
                        combined_docs.append(doc)
                        seen_contents.add(doc.page_content)
                
                if i < len(bm25_docs):
                    doc = bm25_docs[i]
                    if doc.page_content not in seen_contents:
                        combined_docs.append(doc)
                        seen_contents.add(doc.page_content)
            
            # ìƒìœ„ 7ê°œ ì„ íƒ
            final_docs = combined_docs[:7]
            
            # ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
            context_list = []
            sources = set()
            
            for doc in final_docs:
                filename = os.path.basename(doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"))
                sources.add(filename)
                context_list.append(f"[[ì¶œì²˜: {filename}]]\n{doc.page_content}")

            context = "\n\n".join(context_list)
            sorted_sources = sorted(list(sources))

            # [ìˆ˜ì •ë¨] í”„ë¡¬í”„íŠ¸ì— ê°€ë…ì„± ê´€ë ¨ ì§€ì‹œì‚¬í•­ ì¶”ê°€
            prompt = f"""
            ë‹¹ì‹ ì€ ì•„ì£¼ëŒ€í•™êµ êµê³¼ê³¼ì • ì „ë¬¸ AI ì¡°êµìž…ë‹ˆë‹¤. 
            ì•„ëž˜ [Context]ì— ìžˆëŠ” ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            
            [Context]
            {context}
            
            [Question]
            {query}
            
            [ì§€ì¹¨]
            1. ë‹µë³€ì€ **ê°€ë…ì„±** ìžˆê²Œ ìž‘ì„±í•˜ì„¸ìš”.
            2. ë‚˜ì—´ë˜ëŠ” ì •ë³´ëŠ” ë°˜ë“œì‹œ **ë¶ˆë¦¿ í¬ì¸íŠ¸(-)**ë‚˜ **ìˆ«ìž ë¦¬ìŠ¤íŠ¸**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤„ë°”ê¿ˆì„ í•˜ì„¸ìš”.
            3. í•µì‹¬ í‚¤ì›Œë“œë‚˜ ê³¼ëª©ëª…ì€ **êµµê²Œ(Bold)** í‘œì‹œí•˜ì„¸ìš”. (ì˜ˆ: **í•´ì„ê°œë¡ **)
            4. Contextì— ì„œë¡œ ë‹¤ë¥¸ ì—°ë„(ì˜ˆ: 2021ë…„, 2024ë…„)ê°€ ìžˆë‹¤ë©´ **ìµœì‹  ì—°ë„**ë¥¼ ìš°ì„ í•˜ì„¸ìš”.
            5. ë‹µë³€ ëì— 'ì°¸ê³  ìžë£Œ: [íŒŒì¼ëª…]'ì„ ëª…ì‹œí•˜ì„¸ìš”.
            """
            
            response = self.llm.invoke(prompt)
            
            # í† í° ì •ì‚° ì¶œë ¥
            usage = response.response_metadata.get('token_usage', {})
            total = usage.get('total_tokens', 0)
            cost = total * 0.00025
            print(f"\n ðŸ’° [í† í° ì •ì‚°] í•©ê³„: {total} (ì•½ {cost:.2f}ì›)")

            return {
                "answer": response.content,
                "sources": sorted_sources,
                "context": context
            }
            
        except Exception as e:
            print(f" [ì˜¤ë¥˜] ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
            return {"answer": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "sources": [], "context": ""}