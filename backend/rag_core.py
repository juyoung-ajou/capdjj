# backend/rag_core.py

import os
# í™˜ê²½ ë³€ìˆ˜ ë¡œë”©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import chromadb

# .env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
load_dotenv()

class RAGService:
    def __init__(self):
        print(" [ì‹œìŠ¤í…œ] OpenAI ëª¨ë¸ ë° ì„ë² ë”© ë¡œë”© ì¤‘...")
        
        # 1. ì„ë² ë”© ëª¨ë¸ (ê²€ìƒ‰ìš©) - ë¬´ë£Œ ëª¨ë¸ ìœ ì§€
        self.embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
        
        # 2. LLM ëª¨ë¸ (ë‹µë³€ìš©) - OpenAI gpt-4o-mini
        # temperature=0: ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ ì°½ì˜ì„± ë„ê¸°
        self.llm = ChatOpenAI(
            model="gpt-4o-mini", 
            temperature=0
        )
        
        self.persist_directory = "chroma_db"
        self.collection_name = "rag_collection"
        
        self.client = chromadb.PersistentClient(path=self.persist_directory)
        self.vector_store = None

        try:
            self.client.get_collection(name=self.collection_name)
            self.vector_store = Chroma(
                client=self.client,
                collection_name=self.collection_name,
                embedding_function=self.embeddings,
            )
            print(f" [ì‹œìŠ¤í…œ] ê¸°ì¡´ ë²¡í„° DB ('{self.collection_name}')ë¥¼ ì—°ê²°í–ˆìŠµë‹ˆë‹¤.")
        except Exception:
            print(f" [ì˜¤ë¥˜] ë²¡í„° DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. python build_db.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    def get_answer(self, query: str):
        if self.vector_store is None:
            return {"answer": "DBê°€ ì—†ìŠµë‹ˆë‹¤.", "sources": [], "context": ""}
        
        # [ë³€ê²½ 1] ë¬¸ì„œ ê²€ìƒ‰ ê°œìˆ˜ë¥¼ 5ê°œ -> 7ê°œë¡œ ëŠ˜ë ¤ ë¹„êµêµ° í™•ë³´
        retrieved_docs = self.vector_store.similarity_search(query, k=7)
        
        # [ë³€ê²½ 2] AIê°€ ì—°ë„ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ [[ì¶œì²˜: íŒŒì¼ëª…]]ì„ ë‚´ìš© ì•ì— ë¶™ì—¬ì¤Œ
        context_list = []
        sources = set()
        
        for doc in retrieved_docs:
            filename = os.path.basename(doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"))
            sources.add(filename)
            # ì˜ˆ: "[[ì¶œì²˜: 2024_ìš”ëŒ.pdf]] ë¬¸ì„œ ë‚´ìš©..."
            context_list.append(f"[[ì¶œì²˜: {filename}]]\n{doc.page_content}")

        context = "\n\n".join(context_list)
        sorted_sources = sorted(list(sources))

        # [ë³€ê²½ 3] í”„ë¡¬í”„íŠ¸ì— 'ìµœì‹  ì—°ë„ ìš°ì„ ' ê·œì¹™ ì¶”ê°€
        prompt = f"""
        ë‹¹ì‹ ì€ ì•„ì£¼ëŒ€í•™êµ êµê³¼ê³¼ì • ì „ë¬¸ AI ì¡°êµì…ë‹ˆë‹¤. 
        ì•„ë˜ [Context]ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        
        [Context]
        {context}
        
        [Question]
        {query}
        
        [ì¤‘ìš” ì§€ì¹¨]
        1. Contextì— ì„œë¡œ ë‹¤ë¥¸ ì—°ë„(ì˜ˆ: 2021ë…„, 2024ë…„)ì˜ ìë£Œê°€ ìˆë‹¤ë©´, ë°˜ë“œì‹œ **ê°€ì¥ ìµœì‹  ì—°ë„ì˜ íŒŒì¼** ë‚´ìš©ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        2. ê³¼ê±° ìë£Œì™€ ë‚´ìš©ì´ ë‹¬ë¼ì¡Œë‹¤ë©´, "2024ë…„ ê¸°ì¤€ìœ¼ë¡œëŠ” ~ì…ë‹ˆë‹¤. (2021ë…„ì—ëŠ” ~ì˜€ìŠµë‹ˆë‹¤)"ë¼ê³  ë¹„êµí•´ì£¼ë©´ ì¢‹ìŠµë‹ˆë‹¤.
        3. Contextì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ê³ , ëª¨ë¥´ë©´ "ì œê³µëœ ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
        4. í•™ì , ê³¼ëª©ëª… ë“± ìˆ˜ì¹˜ëŠ” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”.
        5. ë‹µë³€ ëì— 'ì°¸ê³  ìë£Œ: [íŒŒì¼ëª…]'ì„ ëª…ì‹œí•˜ì„¸ìš”.
        """
        
        # ë‹µë³€ ìƒì„±
        response = self.llm.invoke(prompt)
        
        # [ë³€ê²½ 4] í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ê³„ì‚° (í„°ë¯¸ë„ ì¶œë ¥ìš©)
        usage = response.response_metadata.get('token_usage', {})
        total_tokens = usage.get('total_tokens', 0)
        # gpt-4o-mini ê¸°ì¤€ ëŒ€ëµì  ì›í™” í™˜ì‚° (í™˜ìœ¨ ë“± ë³€ë™ ê°€ëŠ¥, ì°¸ê³ ìš©)
        cost_krw = total_tokens * 0.00025 
        
        print("\n" + "="*40)
        print(f" ğŸ’° [í† í° ì •ì‚° - gpt-4o-mini]")
        print(f" - ì…ë ¥(ì§ˆë¬¸+ë¬¸ì„œ): {usage.get('prompt_tokens')} í† í°")
        print(f" - ì¶œë ¥(ë‹µë³€): {usage.get('completion_tokens')} í† í°")
        print(f" - í•©ê³„: {total_tokens} í† í° (ì•½ {cost_krw:.2f}ì›)")
        print("="*40 + "\n")
        
        # ê²°ê³¼ ë°˜í™˜
        return {
            "answer": response.content,
            "sources": sorted_sources,
            "context": context
        }