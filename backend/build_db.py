# backend/build_db.py
import os
import glob
import re
from typing import Optional
import pdfplumber
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import chromadb

# --- ì„¤ì • ---
PDF_SOURCE_DIR = "pdf_documents"
PERSIST_DIRECTORY = "chroma_db"
COLLECTION_NAME = "rag_collection"
CHUNK_SIZE = 1000 
CHUNK_OVERLAP = 100
# --- ì„¤ì • ë ---

def pdf_to_markdown(pdf_path):
    """
    í‘œì™€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì¶”ì¶œí•˜ì—¬ AIê°€ ë¬¸ë§¥ì„ ë†“ì¹˜ì§€ ì•Šê²Œ í•¨
    """
    full_text = ""
    
    # í‘œ ì¶”ì¶œ ì„¤ì • (ë³µì¡í•œ í‘œë„ ì˜ ì¡ë„ë¡ ì„¤ì •ê°’ íŠœë‹)
    table_settings = {
        "vertical_strategy": "lines", 
        "horizontal_strategy": "lines",
        "snap_tolerance": 4,
        "intersection_x_tolerance": 5,
        "intersection_y_tolerance": 5,
    }

    try:
        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages):
                page_text = ""
                
                # 1. [êµ¬ì¡°í™” ë°ì´í„°] í‘œ ì¶”ì¶œ ì‹œë„
                tables = page.extract_tables(table_settings)
                
                if tables:
                    print(f"  [p.{i+1}] ğŸ“„ í‘œ {len(tables)}ê°œ ë°œê²¬ (Markdown ë³€í™˜)")
                    for table in tables:
                        # None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì¹˜í™˜
                        cleaned_table = [
                            [str(cell).replace('\n', ' ') if cell is not None else "" for cell in row]
                            for row in table
                        ]
                        if not cleaned_table: continue

                        # ë§ˆí¬ë‹¤ìš´ í‘œ ìƒì„±
                        headers = cleaned_table[0]
                        markdown_table = "\n\n| " + " | ".join(headers) + " |\n"
                        markdown_table += "| " + " | ".join(["---"] * len(headers)) + " |\n"
                        for row in cleaned_table[1:]:
                            markdown_table += "| " + " | ".join(row) + " |\n"
                        markdown_table += "\n"
                        page_text += markdown_table

                # 2. [ì‹œê°ì  ë°ì´í„°] í…ìŠ¤íŠ¸ ë ˆì´ì•„ì›ƒ ì¶”ì¶œ (í•µì‹¬! â­)
                # í‘œê°€ ìˆë“  ì—†ë“  ë¬´ì¡°ê±´ ì›ë³¸ ìœ„ì¹˜ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆ ë” ì €ì¥í•©ë‹ˆë‹¤.
                # ì´ë ‡ê²Œ í•˜ë©´ ë³‘í•©ëœ ì…€ ë•Œë¬¸ì— í‘œê°€ ê¹¨ì ¸ë„, AIê°€ ì´ í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  ì •ë‹µì„ ì°¾ìŠµë‹ˆë‹¤.
                raw_text_layout = page.extract_text(layout=True)
                if raw_text_layout:
                    page_text += f"\n\n[ì›ë³¸ í…ìŠ¤íŠ¸ ë ˆì´ì•„ì›ƒ]\n{raw_text_layout}\n"

                # í˜ì´ì§€ ë²ˆí˜¸ì™€ í•¨ê»˜ ì €ì¥
                full_text += f"\n[[í˜ì´ì§€: {i+1}]]\n{page_text}"
                
    except Exception as e:
        print(f"Error reading {pdf_path}: {e}")
        return None

    return full_text

def extract_department(filename: str) -> Optional[str]:
    # Extract first "<Korean>í•™ê³¼" from filename.
    match = re.search(r"([ê°€-í£]+í•™ê³¼)", filename)
    return match.group(1) if match else None

def build_vector_db():
    print("="*50)
    if not os.path.isdir(PDF_SOURCE_DIR):
        print(f"'{PDF_SOURCE_DIR}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    pdf_files = glob.glob(os.path.join(PDF_SOURCE_DIR, "*.pdf"))
    if not pdf_files:
        print("PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, 
        chunk_overlap=CHUNK_OVERLAP
    )
    embeddings = HuggingFaceEmbeddings(model_name="./my_finetuned_model")
    
    all_docs = []
    
    for pdf_path in pdf_files:
        filename = os.path.basename(pdf_path)
        print(f"Processing: {filename} ...")
        
        markdown_text = pdf_to_markdown(pdf_path)
        
        if markdown_text:
            department = extract_department(filename)
            metadata = {"source": filename}
            if department:
                metadata["department"] = department
            raw_doc = Document(page_content=markdown_text, metadata=metadata)
            docs = text_splitter.split_documents([raw_doc])
            all_docs.extend(docs)
            print(f" -> {len(docs)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

    if not all_docs:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\n" + "="*50)
    print(f"ì´ {len(all_docs)}ê°œì˜ ì²­í¬ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤...")

    client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)
    try:
        client.delete_collection(name=COLLECTION_NAME)
    except:
        pass

    Chroma.from_documents(
        documents=all_docs, 
        embedding=embeddings,
        collection_name=COLLECTION_NAME,
        client=client
    )
    
    print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ! (í‘œ êµ¬ì¡°ì™€ ì‹œê°ì  ë°°ì¹˜ë¥¼ ëª¨ë‘ í•™ìŠµí–ˆìŠµë‹ˆë‹¤)")
    print("="*50)

if __name__ == "__main__":
    build_vector_db()
